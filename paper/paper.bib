@MISC{Black2022-or,
  title     = "{EDAM}: the bioscientific data analysis ontology (update 2021)",
  author    = "Black, Melissa and Lamothe, Lucie and Eldakroury, Hager and
               Kierkegaard, Mads and Priya, Ankita and Machinda, Anne and
               Khanduja, Uttam Singh and Patoliya, Drashti and Rathi, Rashika
               and Nico, Tawah Peggy Che and Umutesi, Gloria and Blankenburg,
               Claudia and Op, Anita and Chieke, Precious and Babatunde,
               Omodolapo and Laurie, Steve and Neumann, Steffen and Schwämmle,
               Veit and Kuzmin, Ivan and Hunter, Chris and Karr, Jonathan and
               Ison, Jon and Gaignard, Alban and Brancotte, Bryan and Ménager,
               Hervé and Kalaš, Matúš",
  journal   = "F1000Research",
  publisher = "F1000 Research Limited",
  volume    =  11,
  abstract  = "Homepage: https://edamontology.org/ Source Code:
               https://github.com/edamontology/edamontology License: CC BY-SA
               4.0 EDAM [1] is a domain ontology of data analysis and data
               management in bio- and other sciences, and science-based
               applications. It comprises concepts related to analysis,
               modelling, optimisation, and data life-cycle. Targetting
               usability by diverse end users, the structure of EDAM is
               relatively simple, divided into 4 main sections: Topic Operation
               Data (incl. Identifier) Format EDAM is used in numerous
               resources, for example Bio.tools, Galaxy, Debian, or the ELIXIR
               Europe training portal TeSS. Thanks to the annotations with EDAM,
               computational tools, workflows, standards, data, and learning
               materials are easier to find, compare, choose, and integrate.
               EDAM contributes to open science by allowing semantic annotation
               of processed data, thus making the data more understandable,
               findable, and comparable. EDAM and its applications lower the
               barrier and effort for scientists and citizens alike, towards
               doing scientific research in a more open, reliable, and inclusive
               way. The main improvements in 2021 include: The addition of
               essential concepts of data management and open science Improved
               automated validation (CI) Improved contribution processes towards
               more inclusion and engagement with communities of scientific
               experts, software engineers, and volunteers Easier to contribute,
               and therefore new contributors and new collaborations. And
               welcoming more! News from the applications: Galaxy can now
               visualise the tools panel sorted by EDAM. The EDAM community
               brings together software engineers and science experts (academic,
               industrial, citizen), professionals and volunteers. It can be
               followed and contributed to via various channels. One option is
               GitHub, with dedicated repositories incl. edamontology and
               others. Other platforms for contributions to EDAM are especially
               the generic ontology browsers of the NCBO BioPortal (EDAM, EDAM
               Bioimaging [2]) and WebProtégé (EDAM, EDAM Bioimaging; free
               registration required also for viewing). The main communication
               channel is Gitter. Along with the ontology, the community has
               developed a number of tools that enhance user experience with
               EDAM: EDAM Browser [3] is a lightweight and fast web-based
               ontology browser that provides a number of user-oriented features
               such as aggregated search across various EDAM-annotated resources
               (e.g. Bio.tools, TeSS), and suggesting changes to EDAM. EDAMmap
               is a tool for text mining EDAM concepts from articles. EDAM
               Popovers is a web-browser add-on/extension for showing details of
               EDAM concepts found in a website. Great e.g. for code and textual
               data on GitHub References: [1] Ison, J., Kalaš, M., Jonassen, I.,
               Bolser, D., Uludag, M., McWilliam, H., Malone, J., Lopez, R.,
               Pettifer, S. and Rice, P. (2013). EDAM: an ontology of
               bioinformatics operations, types of data and identifiers, topics
               and formats. Bioinformatics, 29(10): 1325-1332. DOI:
               10.1093/bioinformatics/btt113 Open access [2] Matúš Kalaš, Laure
               Plantard, Joakim Lindblad, Martin Jones, Nataša Sladoje, Moritz
               A. Kirschmann, Anatole Chessel, Leandro Scholz, Fabienne Rössler,
               Laura Nicolás Sáenz, Estibaliz Gómez de Mariscal, John Bogovic,
               Alexandre Dufour, Xavier Heiligenstein, Dominic Waithe,
               Marie-Charlotte Domart, Matthia Karreman, Raf Van de Plas, Robert
               Haase, David Hörl, Lassi Paavolainen, Ivana Vrhovac Madunić, Dean
               Karaica, Arrate Muñoz-Barrutia, Paula Sampaio, Daniel Sage,
               Sebastian Munck, Ofra Golani, Josh Moore, Florian Levet, Jon
               Ison, Alban Gaignard, Hervé Ménager, Chong Zhang, Kota Miura,
               Julien Colombelli, Perrine Paul-Gilloteaux, and welcoming new
               contributors! (2020). EDAM-bioimaging: the ontology of bioimage
               informatics operations, topics, data, and formats (update 2020)
               [version 1; not peer reviewed]. F1000Research, 9(ELIXIR):162
               (Poster) DOI: 10.7490/f1000research.1117826.1 Open access [3]
               Brancotte, B., Blanchet, C. and Ménager, H. (2018). A reusable
               tree-based web-visualization to browse EDAM ontology, and
               contribute to it. J. Open Source Softw., 3(27): 698. DOI:
               10.21105/joss.00698 Open access Acknowledgement: EDAM maintainers
               and interns were supported by ELIXIR Europe, Norway, and France.
               Dr. Melissa Black and Gloria Umutesi were awarded with
               complimentary registration at ISMB/ECCB 2021, funded by the BOSC
               2021 sponsors. The list of co-authors includes the substantial
               contributors to EDAM in 2020-2021 (version 1.26), without the
               contributors specific to EDAM Biomaging, EDAM Browser, or
               EDAMmap.",
  month     =  jan,
  year      =  2022
}

@ARTICLE{Leo2024-wa,
  title     = "Recording provenance of workflow runs with {RO}-Crate",
  author    = "Leo, Simone and Crusoe, Michael R and Rodríguez-Navas, Laura and
               Sirvent, Raül and Kanitz, Alexander and De Geest, Paul and
               Wittner, Rudolf and Pireddu, Luca and Garijo, Daniel and
               Fernández, José M and Colonnelli, Iacopo and Gallo, Matej and
               Ohta, Tazro and Suetake, Hirotaka and Capella-Gutierrez, Salvador
               and de Wit, Renske and Kinoshita, Bruno P and Soiland-Reyes,
               Stian",
  journal   = "PLoS One",
  publisher = "Public Library of Science (PLoS)",
  volume    =  19,
  number    =  9,
  pages     = "e0309210",
  abstract  = "Recording the provenance of scientific computation results is key
               to the support of traceability, reproducibility and quality
               assessment of data products. Several data models have been
               explored to address this need, providing representations of
               workflow plans and their executions as well as means of packaging
               the resulting information for archiving and sharing. However,
               existing approaches tend to lack interoperable adoption across
               workflow management systems. In this work we present Workflow Run
               RO-Crate, an extension of RO-Crate (Research Object Crate) and
               Schema.org to capture the provenance of the execution of
               computational workflows at different levels of granularity and
               bundle together all their associated objects (inputs, outputs,
               code, etc.). The model is supported by a diverse, open community
               that runs regular meetings, discussing development, maintenance
               and adoption aspects. Workflow Run RO-Crate is already
               implemented by several workflow management systems, allowing
               interoperable comparisons between workflow runs from
               heterogeneous systems. We describe the model, its alignment to
               standards such as W3C PROV, and its implementation in six
               workflow systems. Finally, we illustrate the application of
               Workflow Run RO-Crate in two use cases of machine learning in the
               digital image analysis domain.",
  month     =  sep,
  year      =  2024,
  language  = "en"
}

@ARTICLE{Langer2024-pd,
  title    = "Empowering bioinformatics communities with Nextflow and nf-core",
  author   = "Langer, Björn E and Amaral, Andreia and Baudement, Marie-Odile and
              Bonath, Franziska and Charles, Mathieu and Chitneedi, Praveen
              Krishna and Clark, Emily L and Di Tommaso, Paolo and Djebali,
              Sarah and Ewels, Philip A and Eynard, Sonia and Yates, James A
              Fellows and Fischer, Daniel and Floden, Evan W and Foissac,
              Sylvain and Gabernet, Gisela and Garcia, Maxime U and Gillard,
              Gareth and Gundappa, Manu Kumar and Guyomar, Cervin and Hakkaart,
              Christopher and Hanssen, Friederike and Harrison, Peter W and
              Hörtenhuber, Matthias and Kurylo, Cyril and Kühn, Christa and
              Lagarrigue, Sandrine and Lallias, Delphine and Macqueen, Daniel J
              and Miller, Edmund and Mir-Pedrol, Júlia and Moreira, Gabriel
              Costa Monteiro and Nahnsen, Sven and Patel, Harshil and Peltzer,
              Alexander and Pitel, Frederique and Ramayo-Caldas, Yuliaxis and da
              Câmara Ribeiro-Dantas, Marcel and Rocha, Dominique and Salavati,
              Mazdak and Sokolov, Alexey and Espinosa-Carrasco, Jose and
              Notredame, Cedric and {the nf-core community.}",
  journal  = "bioRxiv",
  abstract = "AbstractStandardised analysis pipelines are an important part of
              FAIR bioinformatics research. Over the last decade, there has been
              a notable shift from point-and-click pipeline solutions such as
              Galaxy towards command-line solutions such as Nextflow and
              Snakemake. We report on recent developments in the nf-core and
              Nextflow frameworks that have led to widespread adoption across
              many scientific communities. We describe how adopting nf-core
              standards enables faster development, improved interoperability,
              and collaboration with the >8,000 members of the nf-core
              community. The recent development of Nextflow Domain-Specific
              Language 2 (DSL2) allows pipeline components to be shared and
              combined across projects. The nf-core community has harnessed this
              with a library of modules and subworkflows that can be integrated
              into any Nextflow pipeline, enabling research communities to
              progressively transition to nf-core best practices. We present a
              case study of nf-core adoption by six European research consortia,
              grouped under the EuroFAANG umbrella and dedicated to farmed
              animal genomics. We believe that the process outlined in this
              report can inspire many large consortia to seek harmonisation of
              their data analysis procedures.",
  month    =  may,
  year     =  2024
}

@ARTICLE{Ewels2020-dj,
  title     = "The nf-core framework for community-curated bioinformatics
               pipelines",
  author    = "Ewels, Philip A and Peltzer, Alexander and Fillinger, Sven and
               Patel, Harshil and Alneberg, Johannes and Wilm, Andreas and
               Garcia, Maxime Ulysse and Di Tommaso, Paolo and Nahnsen, Sven",
  journal   = "Nat. Biotechnol.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  38,
  number    =  3,
  pages     = "276--278",
  month     =  mar,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Soiland-Reyes2022-yh,
  title     = "Packaging research artefacts with {RO}-Crate",
  author    = "Soiland-Reyes, Stian and Sefton, Peter and Crosas, Mercè and
               Castro, Leyla Jael and Coppens, Frederik and Fernández, José M
               and Garijo, Daniel and Grüning, Björn and La Rosa, Marco and Leo,
               Simone and Ó Carragáin, Eoghan and Portier, Marc and Trisovic,
               Ana and {RO-Crate Community} and Groth, Paul and Goble, Carole",
  journal   = "Data Sci.",
  publisher = "IOS Press",
  volume    =  5,
  number    =  2,
  pages     = "97--138",
  abstract  = "An increasing number of researchers support reproducibility by
               including pointers to and descriptions of datasets, software and
               methods in their publications. However, scientific articles may
               be ambiguous, incomplete and difficult to process by automated
               systems. In this paper we introduce RO-Crate, an open,
               community-driven, and lightweight approach to packaging research
               artefacts along with their metadata in a machine readable manner.
               RO-Crate is based on Schema.org annotations in JSON-LD, aiming to
               establish best practices to formally describe metadata in an
               accessible and practical way for their use in a wide variety of
               situations. An RO-Crate is a structured archive of all the items
               that contributed to a research outcome, including their
               identifiers, provenance, relations and annotations. As a general
               purpose packaging approach for data and their metadata, RO-Crate
               is used across multiple areas, including bioinformatics, digital
               humanities and regulatory sciences. By applying “just enough”
               Linked Data standards, RO-Crate simplifies the process of making
               research outputs FAIR while also enhancing research
               reproducibility. An RO-Crate for this article11
               https://w3id.org/ro/doi/10.5281/zenodo.5146227 is archived at
               https://doi.org/10.5281/zenodo.5146227.",
  month     =  jul,
  year      =  2022
}

@ARTICLE{Bechhofer2013-wj,
  title     = "Why linked data is not enough for scientists",
  author    = "Bechhofer, Sean and Buchan, Iain and De Roure, David and Missier,
               Paolo and Ainsworth, John and Bhagat, Jiten and Couch, Philip and
               Cruickshank, Don and Delderfield, Mark and Dunlop, Ian and
               Gamble, Matthew and Michaelides, Danius and Owen, Stuart and
               Newman, David and Sufi, Shoaib and Goble, Carole",
  journal   = "Future Gener. Comput. Syst.",
  publisher = "Elsevier BV",
  volume    =  29,
  number    =  2,
  pages     = "599--611",
  abstract  = "Scientific data represents a significant portion of the linked
               open data cloud and scientists stand to benefit from the data
               fusion capability this will afford. Publishing linked data into
               the cloud, however, does not ensure the required reusability.
               Publishing has requirements of provenance, quality, credit,
               attribution and methods to provide the reproducibility that
               enables validation of results. In this paper we make the case for
               a scientific data publication model on top of linked data and
               introduce the notion of Research Objects as first class citizens
               for sharing and publishing.",
  month     =  feb,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Wilkinson2016-ig,
  title    = "The {FAIR} Guiding Principles for scientific data management and
              stewardship",
  author   = "Wilkinson, Mark D and Dumontier, Michel and Aalbersberg, I Jsbrand
              Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and
              Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz
              Bonino and Bourne, Philip E and Bouwman, Jildau and Brookes,
              Anthony J and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and
              Dumon, Olivier and Edmunds, Scott and Evelo, Chris T and Finkers,
              Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J G and
              Groth, Paul and Goble, Carole and Grethe, Jeffrey S and Heringa,
              Jaap and 't Hoen, Peter A C and Hooft, Rob and Kuhn, Tobias and
              Kok, Ruben and Kok, Joost and Lusher, Scott J and Martone, Maryann
              E and Mons, Albert and Packer, Abel L and Persson, Bengt and
              Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and
              Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry
              and Slater, Ted and Strawn, George and Swertz, Morris A and
              Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and
              Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and
              Wolstencroft, Katherine and Zhao, Jun and Mons, Barend",
  journal  = "Sci Data",
  volume   =  3,
  pages    =  160018,
  abstract = "There is an urgent need to improve the infrastructure supporting
              the reuse of scholarly data. A diverse set of
              stakeholders-representing academia, industry, funding agencies,
              and scholarly publishers-have come together to design and jointly
              endorse a concise and measureable set of principles that we refer
              to as the FAIR Data Principles. The intent is that these may act
              as a guideline for those wishing to enhance the reusability of
              their data holdings. Distinct from peer initiatives that focus on
              the human scholar, the FAIR Principles put specific emphasis on
              enhancing the ability of machines to automatically find and use
              the data, in addition to supporting its reuse by individuals. This
              Comment is the first formal publication of the FAIR Principles,
              and includes the rationale behind them, and some exemplar
              implementations in the community.",
  month    =  mar,
  year     =  2016,
  language = "en"
}

@ARTICLE{Burgin2023-ds,
  title    = "The European Nucleotide Archive in 2022",
  author   = "Burgin, Josephine and Ahamed, Alisha and Cummins, Carla and
              Devraj, Rajkumar and Gueye, Khadim and Gupta, Dipayan and Gupta,
              Vikas and Haseeb, Muhammad and Ihsan, Maira and Ivanov, Eugene and
              Jayathilaka, Suran and Balavenkataraman Kadhirvelu, Vishnukumar
              and Kumar, Manish and Lathi, Ankur and Leinonen, Rasko and
              Mansurova, Milena and McKinnon, Jasmine and O'Cathail, Colman and
              Paupério, Joana and Pesant, Stéphane and Rahman, Nadim and Rinck,
              Gabriele and Selvakumar, Sandeep and Suman, Swati and Vijayaraja,
              Senthilnathan and Waheed, Zahra and Woollard, Peter and Yuan,
              David and Zyoud, Ahmad and Burdett, Tony and Cochrane, Guy",
  journal  = "Nucleic Acids Res.",
  volume   =  51,
  number   = "D1",
  pages    = "D121--D125",
  abstract = "The European Nucleotide Archive (ENA; https://www.ebi.ac.uk/ena),
              maintained by the European Molecular Biology Laboratory's European
              Bioinformatics Institute (EMBL-EBI), offers those producing data
              an open and supported platform for the management, archiving,
              publication, and dissemination of data; and to the scientific
              community as a whole, it offers a globally comprehensive data set
              through a host of data discovery and retrieval tools. Here, we
              describe recent updates to the ENA's submission and retrieval
              services as well as focused efforts to improve connectivity,
              reusability, and interoperability of ENA data and metadata.",
  month    =  jan,
  year     =  2023,
  language = "en"
}

@ARTICLE{Richardson2023-ot,
  title    = "{MGnify}: the microbiome sequence data analysis resource in 2023",
  author   = "Richardson, Lorna and Allen, Ben and Baldi, Germana and
              Beracochea, Martin and Bileschi, Maxwell L and Burdett, Tony and
              Burgin, Josephine and Caballero-Pérez, Juan and Cochrane, Guy and
              Colwell, Lucy J and Curtis, Tom and Escobar-Zepeda, Alejandra and
              Gurbich, Tatiana A and Kale, Varsha and Korobeynikov, Anton and
              Raj, Shriya and Rogers, Alexander B and Sakharova, Ekaterina and
              Sanchez, Santiago and Wilkinson, Darren J and Finn, Robert D",
  journal  = "Nucleic Acids Res.",
  volume   =  51,
  number   = "D1",
  pages    = "D753--D759",
  abstract = "The MGnify platform (https://www.ebi.ac.uk/metagenomics)
              facilitates the assembly, analysis and archiving of
              microbiome-derived nucleic acid sequences. The platform provides
              access to taxonomic assignments and functional annotations for
              nearly half a million analyses covering metabarcoding,
              metatranscriptomic, and metagenomic datasets, which are derived
              from a wide range of different environments. Over the past 3
              years, MGnify has not only grown in terms of the number of
              datasets contained but also increased the breadth of analyses
              provided, such as the analysis of long-read sequences. The MGnify
              protein database now exceeds 2.4 billion non-redundant sequences
              predicted from metagenomic assemblies. This collection is now
              organised into a relational database making it possible to
              understand the genomic context of the protein through navigation
              back to the source assembly and sample metadata, marking a major
              improvement. To extend beyond the functional annotations already
              provided in MGnify, we have applied deep learning-based annotation
              methods. The technology underlying MGnify's Application
              Programming Interface (API) and website has been upgraded, and we
              have enabled the ability to perform downstream analysis of the
              MGnify data through the introduction of a coupled Jupyter Lab
              environment.",
  month    =  jan,
  year     =  2023,
  language = "en"
}

@INCOLLECTION{Gray2017-nx,
  title     = "Bioschemas: From Potato Salad Protein Annotation",
  author    = "Gray, J G and Goble, C and Jimenez, R",
  booktitle = "International Semantic Web Conference (Posters, Demos \& Industry
               Tracks)",
  year      =  2017
}

@MISC{Berners-Lee-gx,
  title        = "Tim berners-lee - semantic web",
  author       = "Berners-Lee, Tim",
  howpublished = "\url{https://www.w3.org/2000/Talks/0906-xmlweb-tbl/text.htm}",
  note         = "Accessed: 2024-11-16",
  language     = "en"
}

@PHDTHESIS{Soiland-Reyes2024-qv,
  title  = "{FAIR} Research Objects and Computational Workflows – A Linked Data
            Approach",
  author = "Soiland-Reyes, Stian",
  month  =  aug,
  year   =  2024,
  school = "University of Amsterdam"
}

@MISC{fairscape,
  title       = "fairscape-cli: Data Validation and Packaging utility for
                 sending evidence graphs to {FAIRSCAPE}",
  institution = "Github",
  abstract    = "Data Validation and Packaging utility for sending evidence
                 graphs to FAIRSCAPE - fairscape/fairscape-cli",
  language    = "en"
}

@MISC{shacl,
  title        = "Shapes constraint language ({SHACL})",
  howpublished = "\url{https://www.w3.org/TR/shacl/}",
  note         = "Accessed: 2024-11-16",
  language     = "en"
}

@ARTICLE{Buttigieg2016-xf,
  title     = "The environment ontology in 2016: bridging domains with increased
               scope, semantic density, and interoperation",
  author    = "Buttigieg, Pier Luigi and Pafilis, Evangelos and Lewis, Suzanna E
               and Schildhauer, Mark P and Walls, Ramona L and Mungall,
               Christopher J",
  journal   = "J. Biomed. Semantics",
  publisher = "Springer Science and Business Media LLC",
  volume    =  7,
  number    =  1,
  pages     =  57,
  abstract  = "BACKGROUND: The Environment Ontology (ENVO;
               http://www.environmentontology.org/ ), first described in 2013,
               is a resource and research target for the semantically controlled
               description of environmental entities. The ontology's initial aim
               was the representation of the biomes, environmental features, and
               environmental materials pertinent to genomic and
               microbiome-related investigations. However, the need for
               environmental semantics is common to a multitude of fields, and
               ENVO's use has steadily grown since its initial description. We
               have thus expanded, enhanced, and generalised the ontology to
               support its increasingly diverse applications. METHODS: We have
               updated our development suite to promote expressivity,
               consistency, and speed: we now develop ENVO in the Web Ontology
               Language (OWL) and employ templating methods to accelerate class
               creation. We have also taken steps to better align ENVO with the
               Open Biological and Biomedical Ontologies (OBO) Foundry
               principles and interoperate with existing OBO ontologies.
               Further, we applied text-mining approaches to extract habitat
               information from the Encyclopedia of Life and automatically
               create experimental habitat classes within ENVO. RESULTS:
               Relative to its state in 2013, ENVO's content, scope, and
               implementation have been enhanced and much of its existing
               content revised for improved semantic representation. ENVO now
               offers representations of habitats, environmental processes,
               anthropogenic environments, and entities relevant to
               environmental health initiatives and the global Sustainable
               Development Agenda for 2030. Several branches of ENVO have been
               used to incubate and seed new ontologies in previously
               unrepresented domains such as food and agronomy. The current
               release version of the ontology, in OWL format, is available at
               http://purl.obolibrary.org/obo/envo.owl . CONCLUSIONS: ENVO has
               been shaped into an ontology which bridges multiple domains
               including biomedicine, natural and anthropogenic ecology, 'omics,
               and socioeconomic development. Through continued interactions
               with our users and partners, particularly those performing data
               archiving and sythesis, we anticipate that ENVO's growth will
               accelerate in 2017. As always, we invite further contributions
               and collaboration to advance the semantic representation of the
               environment, ranging from geographic features and environmental
               materials, across habitats and ecosystems, to everyday objects in
               household settings.",
  month     =  sep,
  year      =  2016,
  keywords  = "Anthropogenic environment; Ecosystem; Environmental semantics;
               Habitat; Indoor environment; Ontology; Sustainable development",
  language  = "en"
}

@ARTICLE{Thakur2024-ey,
  title    = "{EMBL}'s European bioinformatics institute ({EMBL}-{EBI}) in 2023",
  author   = "Thakur, Matthew and Buniello, Annalisa and Brooksbank, Catherine
              and Gurwitz, Kim T and Hall, Matthew and Hartley, Matthew and
              Hulcoop, David G and Leach, Andrew R and Marques, Diana and
              Martin, Maria and Mithani, Aziz and McDonagh, Ellen M and
              Mutasa-Gottgens, Euphemia and Ochoa, David and Perez-Riverol,
              Yasset and Stephenson, James and Varadi, Mihaly and Velankar,
              Sameer and Vizcaino, Juan Antonio and Witham, Rick and McEntyre,
              Johanna",
  journal  = "Nucleic Acids Res.",
  volume   =  52,
  number   = "D1",
  pages    = "D10--D17",
  abstract = "The European Molecular Biology Laboratory's European
              Bioinformatics Institute (EMBL-EBI) is one of the world's leading
              sources of public biomolecular data. Based at the Wellcome Genome
              Campus in Hinxton, UK, EMBL-EBI is one of six sites of the
              European Molecular Biology Laboratory (EMBL), Europe's only
              intergovernmental life sciences organisation. This overview
              summarises the latest developments in the services provided by
              EMBL-EBI data resources to scientific communities globally. These
              developments aim to ensure EMBL-EBI resources meet the current and
              future needs of these scientific communities, accelerating the
              impact of open biological data for all.",
  month    =  jan,
  year     =  2024,
  language = "en"
}

@ARTICLE{Huson2007-ju,
  title     = "{MEGAN} analysis of metagenomic data",
  author    = "Huson, Daniel H and Auch, Alexander F and Qi, Ji and Schuster,
               Stephan C",
  journal   = "Genome Res.",
  publisher = "Cold Spring Harbor Laboratory",
  volume    =  17,
  number    =  3,
  pages     = "377--386",
  abstract  = "Metagenomics is the study of the genomic content of a sample of
               organisms obtained from a common habitat using targeted or random
               sequencing. Goals include understanding the extent and role of
               microbial diversity. The taxonomical content of such a sample is
               usually estimated by comparison against sequence databases of
               known sequences. Most published studies use the analysis of
               paired-end reads, complete sequences of environmental fosmid and
               BAC clones, or environmental assemblies. Emerging
               sequencing-by-synthesis technologies with very high throughput
               are paving the way to low-cost random ``shotgun'' approaches.
               This paper introduces MEGAN, a new computer program that allows
               laptop analysis of large metagenomic data sets. In a
               preprocessing step, the set of DNA sequences is compared against
               databases of known sequences using BLAST or another comparison
               tool. MEGAN is then used to compute and explore the taxonomical
               content of the data set, employing the NCBI taxonomy to summarize
               and order the results. A simple lowest common ancestor algorithm
               assigns reads to taxa such that the taxonomical level of the
               assigned taxon reflects the level of conservation of the
               sequence. The software allows large data sets to be dissected
               without the need for assembly or the targeting of specific
               phylogenetic markers. It provides graphical and statistical
               output for comparing different data sets. The approach is applied
               to several data sets, including the Sargasso Sea data set, a
               recently published metagenomic data set sampled from a mammoth
               bone, and several complete microbial genomes. Also, simulations
               that evaluate the performance of the approach for different read
               lengths are presented.",
  month     =  mar,
  year      =  2007,
  language  = "en"
}

@ARTICLE{Morais2022-ja,
  title     = "{MEDUSA}: A pipeline for sensitive taxonomic classification and
               flexible functional annotation of metagenomic shotgun sequences",
  author    = "Morais, Diego A A and Cavalcante, João V F and Monteiro, Shênia S
               and Pasquali, Matheus A B and Dalmolin, Rodrigo J S",
  journal   = "Front. Genet.",
  publisher = "Frontiers Media SA",
  volume    =  13,
  pages     =  814437,
  abstract  = "Metagenomic studies unravel details about the taxonomic
               composition and the functions performed by microbial communities.
               As a complete metagenomic analysis requires different tools for
               different purposes, the selection and setup of these tools remain
               challenging. Furthermore, the chosen toolset will affect the
               accuracy, the formatting, and the functional identifiers reported
               in the results, impacting the results interpretation and the
               biological answer obtained. Thus, we surveyed state-of-the-art
               tools available in the literature, created simulated datasets,
               and performed benchmarks to design a sensitive and flexible
               metagenomic analysis pipeline. Here we present MEDUSA, an
               efficient pipeline to conduct comprehensive metagenomic analyses.
               It performs preprocessing, assembly, alignment, taxonomic
               classification, and functional annotation on shotgun data,
               supporting user-built dictionaries to transfer annotations to any
               functional identifier. MEDUSA includes several tools, as fastp,
               Bowtie2, DIAMOND, Kaiju, MEGAHIT, and a novel tool implemented in
               Python to transfer annotations to BLAST/DIAMOND alignment
               results. These tools are installed via Conda, and the workflow is
               managed by Snakemake, easing the setup and execution. Compared
               with MEGAN 6 Community Edition, MEDUSA correctly identifies more
               species, especially the less abundant, and is more suited for
               functional analysis using Gene Ontology identifiers.",
  month     =  mar,
  year      =  2022,
  keywords  = "bioinformatics; functional annotation; metagenomics; pipeline;
               shotgun sequences; taxonomic classification",
  language  = "en"
}

@ARTICLE{Uritskiy2018-ud,
  title     = "{MetaWRAP}-a flexible pipeline for genome-resolved metagenomic
               data analysis",
  author    = "Uritskiy, Gherman V and DiRuggiero, Jocelyne and Taylor, James",
  journal   = "Microbiome",
  publisher = "Springer Science and Business Media LLC",
  volume    =  6,
  number    =  1,
  pages     =  158,
  abstract  = "BACKGROUND: The study of microbiomes using whole-metagenome
               shotgun sequencing enables the analysis of uncultivated microbial
               populations that may have important roles in their environments.
               Extracting individual draft genomes (bins) facilitates
               metagenomic analysis at the single genome level. Software and
               pipelines for such analysis have become diverse and
               sophisticated, resulting in a significant burden for biologists
               to access and use them. Furthermore, while bin extraction
               algorithms are rapidly improving, there is still a lack of tools
               for their evaluation and visualization. RESULTS: To address these
               challenges, we present metaWRAP, a modular pipeline software for
               shotgun metagenomic data analysis. MetaWRAP deploys
               state-of-the-art software to handle metagenomic data processing
               starting from raw sequencing reads and ending in metagenomic bins
               and their analysis. MetaWRAP is flexible enough to give
               investigators control over the analysis, while still being
               easy-to-install and easy-to-use. It includes hybrid algorithms
               that leverage the strengths of a variety of software to extract
               and refine high-quality bins from metagenomic data through bin
               consolidation and reassembly. MetaWRAP's hybrid bin extraction
               algorithm outperforms individual binning approaches and other bin
               consolidation programs in both synthetic and real data sets.
               Finally, metaWRAP comes with numerous modules for the analysis of
               metagenomic bins, including taxonomy assignment, abundance
               estimation, functional annotation, and visualization.
               CONCLUSIONS: MetaWRAP is an easy-to-use modular pipeline that
               automates the core tasks in metagenomic analysis, while
               contributing significant improvements to the extraction and
               interpretation of high-quality metagenomic bins. The bin
               refinement and reassembly modules of metaWRAP consistently
               outperform other binning approaches. Each module of metaWRAP is
               also a standalone component, making it a flexible and versatile
               tool for tackling metagenomic shotgun sequencing data. MetaWRAP
               is open-source software available at
               https://github.com/bxlab/metaWRAP .",
  month     =  sep,
  year      =  2018,
  keywords  = "Bin; Binning; Draft genome; Metagenome; Metagenomics; Pipeline;
               Reassembly; WGS",
  language  = "en"
}
@ARTICLE{Molder2021-wi,
  title     = "Sustainable data analysis with Snakemake",
  author    = "Mölder, Felix and Jablonski, Kim Philipp and Letcher, Brice and
               Hall, Michael B and Tomkins-Tinch, Christopher H and Sochat,
               Vanessa and Forster, Jan and Lee, Soohyun and Twardziok, Sven O
               and Kanitz, Alexander and Wilm, Andreas and Holtgrewe, Manuel and
               Rahmann, Sven and Nahnsen, Sven and Köster, Johannes",
  journal   = "F1000Res.",
  publisher = "F1000 Research Ltd",
  volume    =  10,
  pages     =  33,
  abstract  = "Data analysis often entails a multitude of heterogeneous steps,
               from the application of various command line tools to the usage
               of scripting languages like R or Python for the generation of
               plots and tables. It is widely recognized that data analyses
               should ideally be conducted in a reproducible way.
               Reproducibility enables technical validation and regeneration of
               results on the original or even new data. However,
               reproducibility alone is by no means sufficient to deliver an
               analysis that is of lasting impact (i.e., sustainable) for the
               field, or even just one research group. We postulate that it is
               equally important to ensure adaptability and transparency. The
               former describes the ability to modify the analysis to answer
               extended or slightly different research questions. The latter
               describes the ability to understand the analysis in order to
               judge whether it is not only technically, but methodologically
               valid. Here, we analyze the properties needed for a data analysis
               to become reproducible, adaptable, and transparent. We show how
               the popular workflow management system Snakemake can be used to
               guarantee this, and how it enables an ergonomic, combined,
               unified representation of all steps involved in data analysis,
               ranging from raw data processing, to quality control and
               fine-grained, interactive exploration and plotting of final
               results.",
  month     =  jan,
  year      =  2021,
  keywords  = "adaptability; data analysis; reproducibility; scalability;
               sustainability; transparency; workflow management",
  language  = "en"
}

@ARTICLE{Crusoe2022-sb,
  title     = "Methods included: standardizing computational reuse and
               portability with the Common Workflow Language",
  author    = "Crusoe, Michael R and Abeln, Sanne and Iosup, Alexandru and
               Amstutz, Peter and Chilton, John and Tijanić, Nebojša and
               Ménager, Hervé and Soiland-Reyes, Stian and Gavrilović, Bogdan
               and Goble, Carole and Community, The Cwl",
  journal   = "Commun. ACM",
  publisher = "Association for Computing Machinery",
  address   = "New York, NY, USA",
  volume    =  65,
  number    =  6,
  pages     = "54--63",
  abstract  = "Standardizing computational reuse and portability with the Common
               Workflow Language.",
  month     =  may,
  year      =  2022
}
@ARTICLE{Di-Tommaso2017-bx,
  title    = "Nextflow enables reproducible computational workflows",
  author   = "Di Tommaso, Paolo and Chatzou, Maria and Floden, Evan W and Barja,
              Pablo Prieto and Palumbo, Emilio and Notredame, Cedric",
  journal  = "Nat. Biotechnol.",
  volume   =  35,
  number   =  4,
  pages    = "316--319",
  month    =  apr,
  year     =  2017,
  language = "en"
}
@MISC{Chadwick2024-va,
  title     = "ro-crate-py",
  author    = "Chadwick, Eli and De Geest, Paul and Droesbeke, Bert and Eguinoa,
               Ignacio and Gaignard, Alban and Hörtenhuber, Matthias and Huber,
               Sebastiaan and Kinoshita, Bruno and Leo, Simone and Pireddu, Luca
               and Rodríguez-Navas, Laura and Sirvent, Raül and Soiland-Reyes,
               Stian",
  publisher = "Zenodo",
  abstract  = "What's Changed Tutorial updates by @elichad in
               https://github.com/ResearchObject/ro-crate-py/pull/180 Add help
               text for some CLI args by @elichad in
               https://github.com/ResearchObject/ro-crate-py/pull/181 Better
               handling of HTTP header by @simleo in
               https://github.com/ResearchObject/ro-crate-py/pull/182 Add
               ro-crate instantiation from a dictionary by @simleo in
               https://github.com/ResearchObject/ro-crate-py/pull/183
               CreativeWorkStatus -> creativeWorkStatus by @mashehu in
               https://github.com/ResearchObject/ro-crate-py/pull/184 Allow
               other entities as values in Entity's properties arg by @simleo in
               https://github.com/ResearchObject/ro-crate-py/pull/192 Add method
               to add actions to the crate by @simleo in
               https://github.com/ResearchObject/ro-crate-py/pull/191 Raise
               exception if trying to set a dict with no id as property value by
               @simleo in https://github.com/ResearchObject/ro-crate-py/pull/193
               Clarify the role of dest\_path in the readme by @simleo in
               https://github.com/ResearchObject/ro-crate-py/pull/194 New
               Contributors @elichad made their first contribution in
               https://github.com/ResearchObject/ro-crate-py/pull/180 Full
               Changelog:
               https://github.com/ResearchObject/ro-crate-py/compare/0.10.0...0.11.0",
  year      =  2024
}

@MISC{Bloemen2024-jb,
  title     = "Creating a Dataverse {RO}-Crate exporter with {FAIR}-{IMPACT}
               support",
  author    = "Bloemen, Dieuwertje and Karadeniz, Özgür",
  publisher = "Zenodo",
  abstract  = "In March 2023, a call was launched by the EOSC FAIR-IMPACT
               project called ``Support offer \#2: Enabling FAIR Signposting and
               RO-Crate for content/metadata discovery and consumption''. We
               decided to join the short-term support action to get expert
               advice on how best to integrate RO-Crate in Dataverse in a
               general and widely applicable way to improve the interoperability
               of data structure and metadata in repositories using the
               Dataverse software.Due to the relatively short time span of the
               FAIR-IMPACT project, we focused on two major aspects; an RO-Crate
               previewer, and a schema.org-based RO-Crate metadata exporter. The
               first was relatively easy to set up, as there was already an
               existing open-source html previewer available. The RO-Crate
               metadata exporter was a bit more challenging as there was first a
               mapping necessary of all metadata elements to schema.org and
               because we wanted to ensure there was some configurability for
               installations that use custom metadata fields in their set-up. We
               decided to stick to the RO-Crate recommendation to use
               schema.org, as by using it whenever possible, we facilitate the
               interoperability with other systems or metadata management tools
               as well.This is a first step in a larger project to fully
               integrate RO-Crate in Dataverse.",
  year      =  2024
}
@misc{fair-ro-crate-in-galaxy,
author = "Paul De Geest",
	title = "Exporting Workflow Run RO-Crates from Galaxy (Galaxy Training Materials)",
	year = "",
	month = "",
	day = "",
	url = "\url{https://training.galaxyproject.org/training-material/topics/fair/tutorials/ro-crate-in-galaxy/tutorial.html}",
	note = "[Online; accessed Tue Nov 26 2024]"
}
@article{Hiltemann_2023,
	doi = {10.1371/journal.pcbi.1010752},
	url = {https://doi.org/10.1371%2Fjournal.pcbi.1010752},
	year = 2023,
	month = {jan},
	publisher = {Public Library of Science ({PLoS})},
	volume = {19},
	number = {1},
	pages = {e1010752},
	author = {Saskia Hiltemann and Helena Rasche and Simon Gladman and Hans-Rudolf Hotz and Delphine Larivi{\`{e}}re and Daniel Blankenberg and Pratik D. Jagtap and Thomas Wollmann and Anthony Bretaudeau and Nadia Gou{\'{e}} and Timothy J. Griffin and Coline Royaux and Yvan Le Bras and Subina Mehta and Anna Syme and Frederik Coppens and Bert Droesbeke and Nicola Soranzo and Wendi Bacon and Fotis Psomopoulos and Crist{\'{o}}bal Gallardo-Alba and John Davis and Melanie Christine Föll and Matthias Fahrner and Maria A. Doyle and Beatriz Serrano-Solano and Anne Claire Fouilloux and Peter van Heusden and Wolfgang Maier and Dave Clements and Florian Heyl and Björn Grüning and B{\'{e}}r{\'{e}}nice Batut and},
	editor = {Francis Ouellette},
	title = {Galaxy Training: A powerful framework for teaching!},
	journal = {PLoS Comput Biol}
}
@ARTICLE{Galaxy-Community2024-ic,
  title     = "The Galaxy platform for accessible, reproducible, and
               collaborative data analyses: 2024 update",
  author    = "{Galaxy Community}",
  journal   = "Nucleic Acids Res.",
  publisher = "Oxford University Press (OUP)",
  volume    =  52,
  number    = "W1",
  pages     = "W83--W94",
  abstract  = "Galaxy (https://galaxyproject.org) is deployed globally,
               predominantly through free-to-use services, supporting
               user-driven research that broadens in scope each year. Users are
               attracted to public Galaxy services by platform stability, tool
               and reference dataset diversity, training, support and
               integration, which enables complex, reproducible, shareable data
               analysis. Applying the principles of user experience design
               (UXD), has driven improvements in accessibility, tool
               discoverability through Galaxy Labs/subdomains, and a redesigned
               Galaxy ToolShed. Galaxy tool capabilities are progressing in two
               strategic directions: integrating general purpose graphical
               processing units (GPGPU) access for cutting-edge methods, and
               licensed tool support. Engagement with global research consortia
               is being increased by developing more workflows in Galaxy and by
               resourcing the public Galaxy services to run them. The Galaxy
               Training Network (GTN) portfolio has grown in both size, and
               accessibility, through learning paths and direct integration with
               Galaxy tools that feature in training courses. Code development
               continues in line with the Galaxy Project roadmap, with
               improvements to job scheduling and the user interface.
               Environmental impact assessment is also helping engage users and
               developers, reminding them of their role in sustainability, by
               displaying estimated CO2 emissions generated by each Galaxy job.",
  month     =  jul,
  year      =  2024,
  language  = "en"
}
@ARTICLE{Rosonovski2024-sg,
  title    = "Europe {PMC} in 2023",
  author   = "Rosonovski, Summer and Levchenko, Maria and Bhatnagar, Rajat and
              Chandrasekaran, Umamageswari and Faulk, Lynne and Hassan, Islam
              and Jeffryes, Matt and Mubashar, Syed Irtaza and Nassar, Maaly and
              Jayaprabha Palanisamy, Madhumiethaa and Parkin, Michael and
              Poluru, Jagadeeswararao and Rogers, Frances and Saha, Shyamasree
              and Selim, Mohamed and Shafique, Zunaira and Ide-Smith, Michele
              and Stephenson, David and Tirunagari, Santosh and Venkatesan,
              Aravind and Xing, Lijun and Harrison, Melissa",
  journal  = "Nucleic Acids Res.",
  volume   =  52,
  number   = "D1",
  pages    = "D1668--D1676",
  abstract = "Europe PMC (https://europepmc.org/) is an open access database of
              life science journal articles and preprints, which contains over
              42 million abstracts and over 9 million full text articles
              accessible via the website, APIs and bulk download. This
              publication outlines new developments to the Europe PMC platform
              since the last database update in 2020 (1) and focuses on five
              main areas. (i) Improving discoverability, reproducibility and
              trust in preprints by indexing new preprint content, enriching
              preprint metadata and identifying withdrawn and removed preprints.
              (ii) Enhancing support for text and data mining by expanding the
              types of annotations provided and developing the Europe PMC
              Annotations Corpus, which can be used to train machine learning
              models to increase their accuracy and precision. (iii) Developing
              the Article Status Monitor tool and email alerts, to notify users
              about new articles and updates to existing records. (iv)
              Positioning Europe PMC as an open scholarly infrastructure through
              increasing the portion of open source core software, improving
              sustainability and accessibility of the service.",
  month    =  jan,
  year     =  2024,
  language = "en"
}
@ARTICLE{Sansone2012-ud,
  title     = "Toward interoperable bioscience data",
  author    = "Sansone, Susanna-Assunta and Rocca-Serra, Philippe and Field,
               Dawn and Maguire, Eamonn and Taylor, Chris and Hofmann, Oliver
               and Fang, Hong and Neumann, Steffen and Tong, Weida and
               Amaral-Zettler, Linda and Begley, Kimberly and Booth, Tim and
               Bougueleret, Lydie and Burns, Gully and Chapman, Brad and Clark,
               Tim and Coleman, Lee-Ann and Copeland, Jay and Das, Sudeshna and
               de Daruvar, Antoine and de Matos, Paula and Dix, Ian and Edmunds,
               Scott and Evelo, Chris T and Forster, Mark J and Gaudet, Pascale
               and Gilbert, Jack and Goble, Carole and Griffin, Julian L and
               Jacob, Daniel and Kleinjans, Jos and Harland, Lee and Haug,
               Kenneth and Hermjakob, Henning and Ho Sui, Shannan J and
               Laederach, Alain and Liang, Shaoguang and Marshall, Stephen and
               McGrath, Annette and Merrill, Emily and Reilly, Dorothy and Roux,
               Magali and Shamu, Caroline E and Shang, Catherine A and
               Steinbeck, Christoph and Trefethen, Anne and Williams-Jones, Bryn
               and Wolstencroft, Katherine and Xenarios, Ioannis and Hide,
               Winston",
  journal   = "Nat. Genet.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  44,
  number    =  2,
  pages     = "121--126",
  abstract  = "To make full use of research data, the bioscience community needs
               to adopt technologies and reward mechanisms that support
               interoperability and promote the growth of an open 'data
               commoning' culture. Here we describe the prerequisites for data
               commoning and present an established and growing ecosystem of
               solutions using the shared 'Investigation-Study-Assay' framework
               to support that vision.",
  month     =  jan,
  year      =  2012,
  language  = "en"
}